{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "import sys\n",
    "sys.path.append(os.environ.get(\"PROJECTDIR\"))\n",
    "from modules_vae.params import VAEParams as specify_params_here\n",
    "from modules_vae.fit import fit\n",
    "from modules_vae.model import MultiModalVAE as Model\n",
    "from modules_vae.predict import predict_to_tsv\n",
    "\n",
    "from utils.dataset import Dataset\n",
    "from utils.parsers import parse_all\n",
    "from utils.splitter import kfold_split\n",
    "from utils.scaler import scale_and_impute_without_train_test_leak as scale_impute\n",
    "from utils.plotlosses import plot_results_to_pdf\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = specify_params_here('os', 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/nus/e1083772/cancer-survival-ml/utils/scaler_external.py:29: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_scaled.fillna(0.0).astype(float)\n",
      "/home/users/nus/e1083772/cancer-survival-ml/utils/scaler_external.py:29: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_scaled.fillna(0.0).astype(float)\n",
      "/home/users/nus/e1083772/cancer-survival-ml/utils/scaler_external.py:29: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_scaled.fillna(0.0).astype(float)\n"
     ]
    }
   ],
   "source": [
    "scratchdir=os.environ.get(\"SPLITDATADIR\")\n",
    "train_features_file=f'{scratchdir}/{params.shuffle}/{params.fold}/train_features_processed.parquet'\n",
    "valid_features_file=f'{scratchdir}/{params.shuffle}/{params.fold}/valid_features_processed.parquet'\n",
    "\n",
    "train_labels_file=f'{scratchdir}/{params.shuffle}/{params.fold}/train_labels.parquet'\n",
    "valid_labels_file=f'{scratchdir}/{params.shuffle}/{params.fold}/valid_labels.parquet'\n",
    "\n",
    "train_features=pd.read_parquet(train_features_file)\n",
    "valid_features=pd.read_parquet(valid_features_file)\n",
    "\n",
    "eventcol = f\"cens{params.endpoint}\"\n",
    "durationcol = f\"{params.endpoint}cdy\"\n",
    "\n",
    "train_labels=pd.read_parquet(train_labels_file)[[eventcol,durationcol]]\n",
    "valid_labels=pd.read_parquet(valid_labels_file)[[eventcol,durationcol]]\n",
    "\n",
    "train_dataframe=pd.concat([train_labels,train_features],axis=1)\n",
    "valid_dataframe=pd.concat([valid_labels,valid_features],axis=1)\n",
    "trainloader = DataLoader(Dataset(train_dataframe, params.input_types_all, event_indicator_col=eventcol,event_time_col=durationcol), batch_size=params.batch_size, shuffle=True)\n",
    "validloader = DataLoader(Dataset(valid_dataframe, params.input_types_all, event_indicator_col=eventcol,event_time_col=durationcol), batch_size=128, shuffle=False)\n",
    "\n",
    "# find column by regex based on input abbrv\n",
    "find_column = {'cth' : 'Feature_chromothripsis',\n",
    "               'apobec': 'Feature_APOBEC',\n",
    "               'clin': 'Feature_clin',\n",
    "               'exp': 'Feature_exp',\n",
    "               'sbs': 'Feature_SBS',\n",
    "               'ig': 'Feature_(RNASeq|SeqWGS)',\n",
    "               'gistic': 'Feature_CNA_(Amp|Del)',\n",
    "               'fish': 'Feature_fish',\n",
    "               'cna': 'Feature_CNA_ENSG'}\n",
    "\n",
    "# lazy determination of input dimensions\n",
    "params.input_dims = [\n",
    "    params.input_dims[i] \n",
    "    if params.input_dims[i] \n",
    "    else train_dataframe.filter(regex=find_column[params.input_types[i]]).columns.__len__() \n",
    "    for i in range(len(params.input_types))\n",
    "]\n",
    "\n",
    "model = Model(params.input_types,\n",
    "            params.input_dims,\n",
    "            params.layer_dims,\n",
    "            params.input_types_subtask,\n",
    "            params.input_dims_subtask,\n",
    "            params.layer_dims_subtask,\n",
    "            params.z_dim)\n",
    "\n",
    "# create output directory\n",
    "os.makedirs(os.path.dirname(params.resultsprefix),exist_ok=True)\n",
    "\n",
    "fit(model, trainloader, validloader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Parse the 3 arguments which we will parallelize across. \n",
    "    the actual hyperparameters to modify are in params.py\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train VAE model. For adjusting hyperparameters, modify params.py')\n",
    "    parser.add_argument('--endpoint', type=str, choices=['pfs', 'os'], default='pfs', help='Survival endpoint (pfs or os)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # comment out these 3 lines if not using PBS\n",
    "    # PBS array ID to override shuffle and fold\n",
    "    _pbs_array_id = int(os.getenv('PBS_ARRAY_INDEX', \"-1\"))\n",
    "    pbs_shuffle=_pbs_array_id%10\n",
    "    pbs_fold=_pbs_array_id//10\n",
    "    params = specify_params_here(args.endpoint, pbs_shuffle, pbs_fold)\n",
    "\n",
    "    scratchdir=os.environ.get(\"SPLITDATADIR\")\n",
    "    train_features_file=f'{scratchdir}/{params.shuffle}/{params.fold}/train_features_processed.parquet'\n",
    "    valid_features_file=f'{scratchdir}/{params.shuffle}/{params.fold}/valid_features_processed.parquet'\n",
    "    \n",
    "    train_labels_file=f'{scratchdir}/{params.shuffle}/{params.fold}/train_labels.parquet'\n",
    "    valid_labels_file=f'{scratchdir}/{params.shuffle}/{params.fold}/valid_labels.parquet'\n",
    "    \n",
    "    train_features=pd.read_parquet(train_features_file)\n",
    "    valid_features=pd.read_parquet(valid_features_file)\n",
    "    \n",
    "    eventcol = f\"cens{params.endpoint}\"\n",
    "    durationcol = f\"{params.endpoint}cdy\"\n",
    "    \n",
    "    train_labels=pd.read_parquet(train_labels_file)[[eventcol,durationcol]]\n",
    "    valid_labels=pd.read_parquet(valid_labels_file)[[eventcol,durationcol]]\n",
    "    \n",
    "    train_dataframe=pd.concat([train_labels,train_features],axis=1)\n",
    "    valid_dataframe=pd.concat([valid_labels,valid_features],axis=1)\n",
    "    trainloader = DataLoader(Dataset(train_dataframe, params.input_types_all, event_indicator_col=eventcol,event_time_col=durationcol), batch_size=params.batch_size, shuffle=True)\n",
    "    validloader = DataLoader(Dataset(valid_dataframe, params.input_types_all, event_indicator_col=eventcol,event_time_col=durationcol), batch_size=128, shuffle=False)\n",
    "    \n",
    "    # find column by regex based on input abbrv\n",
    "    find_column = {'cth' : 'Feature_chromothripsis',\n",
    "                   'apobec': 'Feature_APOBEC',\n",
    "                   'clin': 'Feature_clin',\n",
    "                   'exp': 'Feature_exp',\n",
    "                   'sbs': 'Feature_SBS',\n",
    "                   'ig': 'Feature_(RNASeq|SeqWGS)',\n",
    "                   'gistic': 'Feature_CNA_(Amp|Del)',\n",
    "                   'fish': 'Feature_fish',\n",
    "                   'cna': 'Feature_CNA_ENSG'}\n",
    "    \n",
    "    # lazy determination of input dimensions\n",
    "    params.input_dims = [\n",
    "        params.input_dims[i] \n",
    "        if params.input_dims[i] \n",
    "        else train_dataframe.filter(regex=find_column[params.input_types[i]]).columns.__len__() \n",
    "        for i in range(len(params.input_types))\n",
    "    ]\n",
    "    \n",
    "    model = Model(params.input_types,\n",
    "                params.input_dims,\n",
    "                params.layer_dims,\n",
    "                params.input_types_subtask,\n",
    "                params.input_dims_subtask,\n",
    "                params.layer_dims_subtask,\n",
    "                params.z_dim)\n",
    "    \n",
    "    # create output directory\n",
    "    os.makedirs(os.path.dirname(params.resultsprefix),exist_ok=True)\n",
    "    \n",
    "    # fit and save history to json\n",
    "    fit(model, trainloader, validloader, params)\n",
    "\n",
    "    # predict on validation data once more and save to tsv\n",
    "    # predict_to_tsv(model, validloader, f'{params.resultsprefix}.tsv', save_embeddings=True)\n",
    "\n",
    "    # plot losses and metrics to pdf\n",
    "    # plot_results_to_pdf(f'{params.resultsprefix}.json',f'{params.resultsprefix}.pdf')\n",
    "\n",
    "    # save model state dict\n",
    "    model.save(f'{params.resultsprefix}.pth')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
